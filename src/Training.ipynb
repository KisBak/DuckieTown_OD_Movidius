{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The teaching of the YOLOv3 image recognition model\n",
    "\n",
    "## Initializing the environment\n",
    "\n",
    "For using the virtualenv environment, please refer to the readme.\n",
    "\n",
    "## Creating the dataset\n",
    "\n",
    "In order to obtain our dataset, we use image augmentation on the following [dataset images](https://github.com/marquezo/darknet/tree/master/duckiestuff).\n",
    "\n",
    "Using the script defined in the __data_fetch__ python file we can create the desired amount of images. Do not forget to __clean your output folders__ before another operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Executing Pipeline:   0%|          | 0/100 [00:00<?, ? Samples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 378 image(s) found.\n",
      "Output directory set to ../data/trainset/../img_aug_trainset."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing <PIL.Image.Image image mode=RGB size=640x480 at 0x7F54185D9250>: 100%|██████████| 100/100 [00:02<00:00, 49.59 Samples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/img_aug_trainset\n"
     ]
    }
   ],
   "source": [
    "# Creating the trainset\n",
    "lib.create_voc_augmented_database(images_dir = \"../data/trainset\", \n",
    "                                  output_dir = \"../data/img_aug_trainset\", \n",
    "                                  label_output_dir = \"../data/lab_aug_trainset\", \n",
    "                                  sample_size = 100)\n",
    "\n",
    "# TODO should we repeat it with the other two directories?\n",
    "# At the evaluation we defenetely shouldn't"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Executing Pipeline:   0%|          | 0/100 [00:00<?, ? Samples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 21 image(s) found.\n",
      "Output directory set to ../data/validset/../img_aug_validset."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing <PIL.Image.Image image mode=RGB size=640x480 at 0x7FAE274FE590>: 100%|██████████| 100/100 [00:02<00:00, 49.29 Samples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/img_aug_validset\n"
     ]
    }
   ],
   "source": [
    "# Creating the validset\n",
    "lib.create_voc_augmented_database(images_dir = \"../data/validset\", \n",
    "                                  output_dir = \"../data/img_aug_validset\", \n",
    "                                  label_output_dir = \"../data/lab_aug_validset\", \n",
    "                                  sample_size = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example training\n",
    "\n",
    "Do not forget to clone your external submodule as well. Then first we copy the proper config file from ext/keras-yolo3/config.json which we will use to set our custom preferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git submodule update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BE CAREFUL, DO NOT OVERWRITE\n",
    "!cp ext/keras-yolo3/config.json ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the configuration\n",
    "\n",
    "Here we have to set the most important settings. They are the followings.\n",
    "\n",
    "__IMPORTANT: CUSTOMIZE IT TO YOUR OWN PATHS__\n",
    "\n",
    "* labels: the labels defined in the xml files, one label belongs to one class\n",
    "* anchors: pair of values describing the windows ratios of the first convolutional layer\n",
    "* train_image_folder: the folder where the training images are stored\n",
    "* train_annot_folder: the folder where the training labels are stored\n",
    "* saved_weights_name: the name of the file where the weights are saved\n",
    "\n",
    "Hints for fine-tuning:\n",
    "generating anchors, ... in [this repo](https://github.com/experiencor/keras-yolo3)\n",
    "\n",
    "Finally do the example training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "[]\n",
      "{}\n",
      "Seen labels: \t{}\n",
      "\n",
      "Given labels: \t['bot', 'duckie', 'greensign', 'yellowsign']\n",
      "Some labels have no annotations! Please revise the list of labels in the config.json.\n",
      "Traceback (most recent call last):\n",
      "  File \"ext/keras-yolo3/train.py\", line 282, in <module>\n",
      "    _main_(args)\n",
      "  File \"ext/keras-yolo3/train.py\", line 184, in _main_\n",
      "    config['model']['labels']\n",
      "ValueError: not enough values to unpack (expected 4, got 3)\n"
     ]
    }
   ],
   "source": [
    "!python ext/keras-yolo3/train.py -c config.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "duckietales",
   "language": "python",
   "name": "duckietales"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
